{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11848,"databundleVersionId":862157,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction and setup","metadata":{}},{"cell_type":"markdown","source":"In this project, we consider a collection of image files. Each file shows a patch of cells from a tissue biopsy, and each file has been labeled by expert diagnosticians according to whether its center region shows signs of metastatic cancer. We are interested in developing a convolutional neural network able to classify novel images of cells according to whether those cells are malignant or benign. We will train two architectures, one with a convolutional block custom-trained with the labeled image files and another with a pretrained feature extraction layer. The following code block imports the libraries, classes, and functions we will use.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom cv2 import imread\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import (\n    Layer, Input, Dense, Conv2D, Flatten, MaxPooling2D, Dropout, \n    BatchNormalization, GlobalAveragePooling2D, LeakyReLU\n)\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.applications import VGG16","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T23:29:00.603963Z","iopub.execute_input":"2025-03-26T23:29:00.604267Z","iopub.status.idle":"2025-03-26T23:29:20.682044Z","shell.execute_reply.started":"2025-03-26T23:29:00.604240Z","shell.execute_reply":"2025-03-26T23:29:20.681118Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"The image files for training and validation are stored in a single directory. The following code creates a data frame `df` from a CSV with image file names and labels.","metadata":{}},{"cell_type":"code","source":"path = \"/kaggle/input/histopathologic-cancer-detection/\"\nfilename = \"train_labels.csv\"\ndf = pd.read_csv(path + filename)\ndf[\"id\"] = df[\"id\"] + \".tif\"\ndf[\"label\"] = df[\"label\"].astype(str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T23:51:31.428997Z","iopub.execute_input":"2025-03-26T23:51:31.429348Z","iopub.status.idle":"2025-03-26T23:51:31.734356Z","shell.execute_reply.started":"2025-03-26T23:51:31.429320Z","shell.execute_reply":"2025-03-26T23:51:31.733666Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Exploratory data analysis","metadata":{}},{"cell_type":"markdown","source":"Before partitioning the images into training and validation sets, we inspect the dimensions and color composition of each image file referenced in `df`.","metadata":{}},{"cell_type":"code","source":"df[\"x\"] = np.zeros((df.shape[0],1))\ndf[\"y\"] = np.zeros((df.shape[0],1))\ndf[\"z\"] = np.zeros((df.shape[0],1))\ndf[\"mean_r\"] = np.zeros((df.shape[0],1))\ndf[\"mean_g\"] = np.zeros((df.shape[0],1))\ndf[\"mean_b\"] = np.zeros((df.shape[0],1))\nfor i in range(df.shape[0]): \n    name = df.iloc[i,][\"id\"]\n    img = imread(path + \"train/\" + name)\n    df.iloc[i,2] = img.shape[0]\n    df.iloc[i,3] = img.shape[1]\n    df.iloc[i,4] = img.shape[2]\n    df.iloc[i,5] = img[:,:,0].mean()\n    df.iloc[i,6] = img[:,:,1].mean()\n    df.iloc[i,7] = img[:,:,2].mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T11:53:55.928295Z","iopub.execute_input":"2025-03-24T11:53:55.928614Z","iopub.status.idle":"2025-03-24T12:28:29.278788Z","shell.execute_reply.started":"2025-03-24T11:53:55.928590Z","shell.execute_reply":"2025-03-24T12:28:29.277319Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"df.loc[:,[\"x\",\"y\",\"z\"]].describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T12:32:15.643524Z","iopub.execute_input":"2025-03-24T12:32:15.644165Z","iopub.status.idle":"2025-03-24T12:32:15.708715Z","shell.execute_reply.started":"2025-03-24T12:32:15.644123Z","shell.execute_reply":"2025-03-24T12:32:15.707614Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"              x         y         z\ncount  220025.0  220025.0  220025.0\nmean       96.0      96.0       3.0\nstd         0.0       0.0       0.0\nmin        96.0      96.0       3.0\n25%        96.0      96.0       3.0\n50%        96.0      96.0       3.0\n75%        96.0      96.0       3.0\nmax        96.0      96.0       3.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>220025.0</td>\n      <td>220025.0</td>\n      <td>220025.0</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>96.0</td>\n      <td>96.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>96.0</td>\n      <td>96.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>96.0</td>\n      <td>96.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>96.0</td>\n      <td>96.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>96.0</td>\n      <td>96.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>96.0</td>\n      <td>96.0</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 3, sharey=True, tight_layout=True)\naxs[0].hist(df[\"mean_r\"])\naxs[1].hist(df[\"mean_g\"])\naxs[2].hist(df[\"mean_b\"])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T12:32:21.071192Z","iopub.execute_input":"2025-03-24T12:32:21.071575Z","iopub.status.idle":"2025-03-24T12:32:21.559949Z","shell.execute_reply.started":"2025-03-24T12:32:21.071546Z","shell.execute_reply":"2025-03-24T12:32:21.558642Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 3 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"Summary statistics for the dimensional data shows that all the images have the same size: $96 \\times 96$ pixels and three color channels. Histograms of the average values for each color channel in each image show approximately normal distributions concentrated in the interval $[0,255]$, as we expect for image files.\n\nNext, we consider whether the distribution of positive and negative labels is balanced.","metadata":{}},{"cell_type":"code","source":"label_counts = df[\"label\"].value_counts()\nplt.pie(label_counts, labels = label_counts.index)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T12:37:17.638323Z","iopub.execute_input":"2025-03-24T12:37:17.638719Z","iopub.status.idle":"2025-03-24T12:37:17.747847Z","shell.execute_reply.started":"2025-03-24T12:37:17.638675Z","shell.execute_reply":"2025-03-24T12:37:17.746663Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"Images of malignant cells are a slight minority in our data a set. In principle, this could introduce a bias toward classifying new images as benign, but we believe such a bias is likely to be negligible. For now, we will maintain the small imbalance when we partition the images.","metadata":{}},{"cell_type":"code","source":"df_train, df_val = train_test_split(df, test_size = 0.2, random_state = 318)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T23:51:41.916755Z","iopub.execute_input":"2025-03-26T23:51:41.917066Z","iopub.status.idle":"2025-03-26T23:51:41.958001Z","shell.execute_reply.started":"2025-03-26T23:51:41.917043Z","shell.execute_reply":"2025-03-26T23:51:41.957304Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"The `ImageDataGenerator` class implements a method `flow_from_dataframe()` for preprocessing image files referenced in a data frame for use by an instance of a Keras `Sequential` model.","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(rescale = 1/255)\ntrain_loader = datagen.flow_from_dataframe(\n    dataframe = df_train,\n    directory = path + \"train/\",\n    x_col = \"id\",\n    y_col = \"label\",\n    class_mode = \"binary\",\n    target_size = (96, 96)\n)\nval_loader = datagen.flow_from_dataframe(\n    dataframe = df_val,\n    directory = path + \"train/\",\n    x_col = \"id\",\n    y_col = \"label\",\n    class_mode = \"binary\",\n    target_size = (96, 96)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T23:51:44.158889Z","iopub.execute_input":"2025-03-26T23:51:44.159179Z","iopub.status.idle":"2025-03-27T00:00:51.726802Z","shell.execute_reply.started":"2025-03-26T23:51:44.159157Z","shell.execute_reply":"2025-03-27T00:00:51.726099Z"}},"outputs":[{"name":"stdout","text":"Found 176020 validated image filenames belonging to 2 classes.\nFound 44005 validated image filenames belonging to 2 classes.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Model 1: Custom convolutional neural net","metadata":{}},{"cell_type":"markdown","source":"For our first model, we will implement a single convolutional block. The documentation accompanying our data set indicates that the images were labeled according to whether their central $32 \\times 32$ pixel regions appeared to have cancerous cells. As a starting point, we will implement our convolutional block with $32$ filters of dimension $3 \\times 3 \\times 3$ so that the output of the final filter represents a $32 \\times 32$ image. Whether to adjust the number of filters or to add additional convolutional blocks is a possible consideration for later fine tuning, but this seems like a reasonable first guess at a good field of view. Between the convolutional layer and hidden layers, we implement batch normalization and a dropout layer to reduce the risk of overfitting, a leaky ReLU activation layer to address potential vanishing gradients. For flattening, we use average pooling to attempt to pass as much information as possible from the convolutional block to the hidden layers.\n\nWe implement two dense hidden layers with $128$ and $64$ nodes and ReLU activation, followed by an output layer with a single node and sigmoid activation for binary classification. To control overfitting further, we implement L2 regularization within each hidden layer, and we implement batch normalization and dropout after each hidden layer.","metadata":{}},{"cell_type":"code","source":"cnn = Sequential([\n    # convolutional block\n    Input((96,96,3)),\n    Conv2D(32, (3, 3), padding='valid'),\n    BatchNormalization(),\n    LeakyReLU(),\n    GlobalAveragePooling2D(),\n    Dropout(0.25),\n    \n    # dense layers\n    Dense(\n        128, \n        activation='relu', \n        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n    ),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(\n        64, \n        activation='relu', \n        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n    ),\n    BatchNormalization(),\n    Dropout(0.5),\n\n    # output layer\n    Dense(1, activation='sigmoid')  # Adjusted for binary classification\n])\ncnn.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:19:19.650519Z","iopub.execute_input":"2025-03-21T02:19:19.650890Z","iopub.status.idle":"2025-03-21T02:19:19.732663Z","shell.execute_reply.started":"2025-03-21T02:19:19.650865Z","shell.execute_reply":"2025-03-21T02:19:19.732026Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m896\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m128\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │           \u001b[38;5;34m4,224\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,224</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,337\u001b[0m (56.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,337</span> (56.00 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,889\u001b[0m (54.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,889</span> (54.25 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n</pre>\n"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"We use `Adam` for optimization, and we implement early stopping to avoid overfitting and improve training efficiency. As this is a binary classification problem, binary cross-entropy is a suitable loss function. We will measure classification performance by the area under the model's ROC curve. (Other classification metrics are possible, but the image data is from a Kaggle competition judeged according to area under ROC curves, so choosing this metric allows for comparison to other models.) For our first iteration of the model, we train in 30 epochs.","metadata":{}},{"cell_type":"code","source":"optimizer = Adam(learning_rate = 0.0001)\nearly_stop = EarlyStopping(\n    monitor = 'val_auc', \n    patience = 5, \n    restore_best_weights = True\n)\ncnn.compile(\n    optimizer = optimizer, \n    loss = 'binary_crossentropy', \n    metrics = [AUC(name = 'auc')]\n)\ncnn_history = cnn.fit(\n    train_loader,                \n    validation_data = val_loader,     \n    epochs = 30,\n    callbacks = [early_stop],\n    verbose = 1\n) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:21:48.547543Z","iopub.execute_input":"2025-03-21T02:21:48.547861Z","iopub.status.idle":"2025-03-21T03:52:42.979146Z","shell.execute_reply.started":"2025-03-21T02:21:48.547835Z","shell.execute_reply":"2025-03-21T03:52:42.978247Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 50ms/step - auc: 0.6918 - loss: 0.8509 - val_auc: 0.8484 - val_loss: 0.5931\nEpoch 2/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 52ms/step - auc: 0.8075 - loss: 0.6347 - val_auc: 0.8513 - val_loss: 0.6510\nEpoch 3/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 52ms/step - auc: 0.8298 - loss: 0.5770 - val_auc: 0.8663 - val_loss: 0.5335\nEpoch 4/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 50ms/step - auc: 0.8444 - loss: 0.5327 - val_auc: 0.8484 - val_loss: 0.5253\nEpoch 5/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 49ms/step - auc: 0.8467 - loss: 0.5135 - val_auc: 0.8669 - val_loss: 0.5353\nEpoch 6/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 49ms/step - auc: 0.8507 - loss: 0.4990 - val_auc: 0.8765 - val_loss: 0.5009\nEpoch 7/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 49ms/step - auc: 0.8551 - loss: 0.4864 - val_auc: 0.8594 - val_loss: 0.6982\nEpoch 8/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 48ms/step - auc: 0.8567 - loss: 0.4795 - val_auc: 0.8825 - val_loss: 0.4549\nEpoch 9/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 49ms/step - auc: 0.8576 - loss: 0.4750 - val_auc: 0.8797 - val_loss: 0.4830\nEpoch 10/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 50ms/step - auc: 0.8600 - loss: 0.4701 - val_auc: 0.8449 - val_loss: 0.4893\nEpoch 11/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 49ms/step - auc: 0.8604 - loss: 0.4669 - val_auc: 0.8798 - val_loss: 0.4507\nEpoch 12/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 51ms/step - auc: 0.8609 - loss: 0.4656 - val_auc: 0.8778 - val_loss: 0.4775\nEpoch 13/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 47ms/step - auc: 0.8627 - loss: 0.4614 - val_auc: 0.8856 - val_loss: 0.4397\nEpoch 14/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 51ms/step - auc: 0.8650 - loss: 0.4571 - val_auc: 0.8841 - val_loss: 0.5531\nEpoch 15/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 49ms/step - auc: 0.8659 - loss: 0.4557 - val_auc: 0.8893 - val_loss: 0.5085\nEpoch 16/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 50ms/step - auc: 0.8631 - loss: 0.4582 - val_auc: 0.8776 - val_loss: 0.4600\nEpoch 17/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 49ms/step - auc: 0.8660 - loss: 0.4533 - val_auc: 0.8831 - val_loss: 0.4640\nEpoch 18/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 48ms/step - auc: 0.8647 - loss: 0.4552 - val_auc: 0.8805 - val_loss: 0.4484\nEpoch 19/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 47ms/step - auc: 0.8664 - loss: 0.4527 - val_auc: 0.8839 - val_loss: 0.4529\nEpoch 20/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 50ms/step - auc: 0.8661 - loss: 0.4526 - val_auc: 0.8762 - val_loss: 0.4477\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"cnn.save(\"cnn.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:52:43.024652Z","iopub.execute_input":"2025-03-21T03:52:43.024971Z","iopub.status.idle":"2025-03-21T03:52:43.068509Z","shell.execute_reply.started":"2025-03-21T03:52:43.024941Z","shell.execute_reply":"2025-03-21T03:52:43.067951Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"Though relatively simple in design, our initial convolutional neural network was able to achieve a maximum area of $0.8893$ under its ROC curve, and training appeared to converge after approximately 15 epochs.","metadata":{}},{"cell_type":"markdown","source":"# Model 2: Pretrained feature extraction layer","metadata":{}},{"cell_type":"markdown","source":"In the first model, we made some educated guesses about a suitable convolutional block structure. In the second model, we use the pretrained `VGG16` model to do feature extraction, the results of which are run through through the same hidden and output layer architecture as in the first model. We wish to see if the pretrained feature extractor can lead to good classification results without any of its own retraining.","metadata":{}},{"cell_type":"code","source":"input_layer = Input((96, 96, 3))\nmodel_vgg16 = VGG16(\n    weights = 'imagenet',\n    input_tensor = input_layer,\n    include_top = False\n)\nmodel_vgg16.trainable = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T23:49:23.246794Z","iopub.execute_input":"2025-03-26T23:49:23.247148Z","iopub.status.idle":"2025-03-26T23:49:23.971262Z","shell.execute_reply.started":"2025-03-26T23:49:23.247123Z","shell.execute_reply":"2025-03-26T23:49:23.970321Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"cnn_pretrained = Sequential([\n    # pretrained feature extraction block\n    Input((96,96,3)),\n    model_vgg16,\n    BatchNormalization(),\n    LeakyReLU(),\n    GlobalAveragePooling2D(),\n    Dropout(0.25),\n    \n    # dense layers\n    Dense(\n        128, \n        activation='relu', \n        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n    ),\n    BatchNormalization(),\n    Dropout(0.5),\n    Dense(\n        64, \n        activation='relu', \n        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n    ),\n    BatchNormalization(),\n    Dropout(0.5),\n\n    # output layer\n    Dense(1, activation='sigmoid')  # Adjusted for binary classification\n])\ncnn_pretrained.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T23:49:27.750710Z","iopub.execute_input":"2025-03-26T23:49:27.751196Z","iopub.status.idle":"2025-03-26T23:49:27.825591Z","shell.execute_reply.started":"2025-03-26T23:49:27.751161Z","shell.execute_reply":"2025-03-26T23:49:27.824949Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │      \u001b[38;5;34m14,714,688\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │           \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu (\u001b[38;5;33mLeakyReLU\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m65,664\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │      <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ leaky_re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,791,489\u001b[0m (56.43 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,791,489</span> (56.43 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m75,393\u001b[0m (294.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">75,393</span> (294.50 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,716,096\u001b[0m (56.14 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,716,096</span> (56.14 MB)\n</pre>\n"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"Even though the convolutional block in the second model is pretrained, there are still many more trainable parameters in the second model than the first. This is due to the fact that `VGG16` applies $512$ filters compared to the $32$ filters in our first model, so there are parameters connecting `VGG16` and the hidden layers than those connecting our custom convolutional block to the hidden layers.\n\nThe large number of additional trainable parameters means that we cannot attribute any classification performance differences between our two models directly to the effectiveness of their convolutional blocks. However, we will train the second model with the same optimization, early stopping, loss function, classification metric, and epoch number as the first to make as close a comparison as possible.","metadata":{}},{"cell_type":"code","source":"optimizer = Adam(learning_rate = 0.0001)\nearly_stop = EarlyStopping(\n    monitor = 'val_auc', \n    patience = 5, \n    restore_best_weights = True\n)\ncnn_pretrained.compile(\n    optimizer = optimizer, \n    loss = 'binary_crossentropy', \n    metrics = [AUC(name = 'auc')]\n)\nhistory_pretrained = cnn_pretrained.fit(\n    train_loader,                \n    validation_data = val_loader,     \n    epochs = 30,\n    callbacks = [early_stop],\n    verbose = 1\n) \ncnn_pretrained.save(\"cnn2.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T23:37:11.458012Z","iopub.execute_input":"2025-03-20T23:37:11.458321Z","iopub.status.idle":"2025-03-21T02:17:54.786520Z","shell.execute_reply.started":"2025-03-20T23:37:11.458284Z","shell.execute_reply":"2025-03-21T02:17:54.785627Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1236s\u001b[0m 223ms/step - auc: 0.7837 - loss: 0.8840 - val_auc: 0.9115 - val_loss: 0.5852\nEpoch 2/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 64ms/step - auc: 0.8791 - loss: 0.6244 - val_auc: 0.9211 - val_loss: 0.4937\nEpoch 3/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 56ms/step - auc: 0.8929 - loss: 0.5301 - val_auc: 0.9268 - val_loss: 0.4322\nEpoch 4/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 53ms/step - auc: 0.9005 - loss: 0.4720 - val_auc: 0.9307 - val_loss: 0.3934\nEpoch 5/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 56ms/step - auc: 0.9037 - loss: 0.4402 - val_auc: 0.9330 - val_loss: 0.3704\nEpoch 6/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 55ms/step - auc: 0.9089 - loss: 0.4152 - val_auc: 0.9349 - val_loss: 0.3573\nEpoch 7/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 56ms/step - auc: 0.9103 - loss: 0.4038 - val_auc: 0.9369 - val_loss: 0.3462\nEpoch 8/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 55ms/step - auc: 0.9113 - loss: 0.3964 - val_auc: 0.9382 - val_loss: 0.3390\nEpoch 9/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 54ms/step - auc: 0.9131 - loss: 0.3887 - val_auc: 0.9393 - val_loss: 0.3343\nEpoch 10/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 53ms/step - auc: 0.9145 - loss: 0.3841 - val_auc: 0.9413 - val_loss: 0.3284\nEpoch 11/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 54ms/step - auc: 0.9149 - loss: 0.3816 - val_auc: 0.9407 - val_loss: 0.3274\nEpoch 12/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 52ms/step - auc: 0.9142 - loss: 0.3818 - val_auc: 0.9414 - val_loss: 0.3245\nEpoch 13/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 50ms/step - auc: 0.9170 - loss: 0.3748 - val_auc: 0.9424 - val_loss: 0.3223\nEpoch 14/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 49ms/step - auc: 0.9175 - loss: 0.3736 - val_auc: 0.9433 - val_loss: 0.3187\nEpoch 15/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 50ms/step - auc: 0.9178 - loss: 0.3724 - val_auc: 0.9436 - val_loss: 0.3195\nEpoch 16/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 57ms/step - auc: 0.9197 - loss: 0.3688 - val_auc: 0.9428 - val_loss: 0.3199\nEpoch 17/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 50ms/step - auc: 0.9182 - loss: 0.3716 - val_auc: 0.9432 - val_loss: 0.3176\nEpoch 18/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 52ms/step - auc: 0.9190 - loss: 0.3695 - val_auc: 0.9436 - val_loss: 0.3165\nEpoch 19/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 55ms/step - auc: 0.9178 - loss: 0.3714 - val_auc: 0.9442 - val_loss: 0.3175\nEpoch 20/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 50ms/step - auc: 0.9183 - loss: 0.3701 - val_auc: 0.9438 - val_loss: 0.3157\nEpoch 21/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 49ms/step - auc: 0.9187 - loss: 0.3695 - val_auc: 0.9442 - val_loss: 0.3149\nEpoch 22/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 52ms/step - auc: 0.9194 - loss: 0.3679 - val_auc: 0.9450 - val_loss: 0.3144\nEpoch 23/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 53ms/step - auc: 0.9208 - loss: 0.3658 - val_auc: 0.9445 - val_loss: 0.3129\nEpoch 24/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 51ms/step - auc: 0.9207 - loss: 0.3661 - val_auc: 0.9447 - val_loss: 0.3128\nEpoch 25/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 49ms/step - auc: 0.9194 - loss: 0.3684 - val_auc: 0.9444 - val_loss: 0.3138\nEpoch 26/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 50ms/step - auc: 0.9203 - loss: 0.3667 - val_auc: 0.9452 - val_loss: 0.3133\nEpoch 27/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 51ms/step - auc: 0.9202 - loss: 0.3673 - val_auc: 0.9451 - val_loss: 0.3129\nEpoch 28/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 51ms/step - auc: 0.9199 - loss: 0.3670 - val_auc: 0.9453 - val_loss: 0.3119\nEpoch 29/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 50ms/step - auc: 0.9212 - loss: 0.3654 - val_auc: 0.9457 - val_loss: 0.3114\nEpoch 30/30\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 50ms/step - auc: 0.9203 - loss: 0.3673 - val_auc: 0.9457 - val_loss: 0.3111\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"cnn_pretrained.save(\"cnn_pretrained.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:18:21.545347Z","iopub.execute_input":"2025-03-21T02:18:21.545638Z","iopub.status.idle":"2025-03-21T02:18:21.828660Z","shell.execute_reply.started":"2025-03-21T02:18:21.545607Z","shell.execute_reply":"2025-03-21T02:18:21.828028Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"The second model had a maximum area of $0.9457$ under its ROC curve, and training lasted the full 30 epochs. Though increases in classification performance seemed to be flattening out, we might have seen additional improvements if we ran the training for a few more epochs. On the other hand, comparing the performance of the two models suggests that pretrained feature extraction is an effective way to boost performance without many rounds of cross-validation to test different convolutional architectures. With that in mind, we proceed to consider hyperparameter tuning for the second model.","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter tuning","metadata":{}},{"cell_type":"markdown","source":"There are numerous hyperparameters to consider adjusting to improve performance of the second model. Examples include\n\n* the number of hidden layers,\n* the number of nodes in each hidden layer,\n* the activation function for each hidden layer,\n* the L2 regularization factor for each hidden layer, and\n* the dropout rates after each hidden layer.\n\nIf time and access to GPUs permitted, we could train many combinations of values for these hyperparameters. For this project, we will focus on the L2 regularization factor and consider three values: $0.0001$, $0.001$, and $0.01$. The following code block contains a function to initialize a model with the same architecture as our second model and a variable L2 regularization factor `l2_param`.","metadata":{}},{"cell_type":"code","source":"def create_cnn(l2_param):\n    cnn_l2 = Sequential([\n        # pretrained feature extraction block\n        Input((96,96,3)),\n        model_vgg16,\n        BatchNormalization(),\n        LeakyReLU(),\n        GlobalAveragePooling2D(),\n        Dropout(0.25),\n        \n        # dense layers\n        Dense(\n            128, \n            activation='relu', \n            kernel_regularizer=tf.keras.regularizers.l2(l2_param)\n        ),\n        BatchNormalization(),\n        Dropout(0.5),\n        Dense(\n            64, \n            activation='relu', \n            kernel_regularizer=tf.keras.regularizers.l2(l2_param)\n        ),\n        BatchNormalization(),\n        Dropout(0.5),\n    \n        # output layer\n        Dense(1, activation='sigmoid')  # Adjusted for binary classification\n    ])\n    return cnn_l2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T04:36:17.197302Z","iopub.execute_input":"2025-03-23T04:36:17.197617Z","iopub.status.idle":"2025-03-23T04:36:17.202590Z","shell.execute_reply.started":"2025-03-23T04:36:17.197586Z","shell.execute_reply":"2025-03-23T04:36:17.201782Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"Examining the training history of the second model, we believe we can get a good idea about how well an L2 regularization factor is performing by looking at just ten epochs. Below, we initialize three models for three factors and train each model over ten epochs.","metadata":{}},{"cell_type":"code","source":"tuning_histories = {}\nfor l2_param in [0.0001, 0.001, 0.01]:\n    cnn_l2 = create_cnn(l2_param)\n    optimizer = Adam(learning_rate = 0.0001)\n    early_stop = EarlyStopping(\n        monitor = 'val_auc', \n        patience = 5, \n        restore_best_weights = True\n    )\n    cnn_l2.compile(\n        optimizer = optimizer, \n        loss = 'binary_crossentropy', \n        metrics = [AUC(name = 'auc')]\n    )\n    history_l2 = cnn_l2.fit(\n        train_loader,                \n        validation_data = val_loader,     \n        epochs = 10,\n        callbacks = [early_stop],\n        verbose = 1\n    )\n    tuning_histories[l2_param] = history_l2\n    cnn_l2.save(\"cnn_l2\" + str(l2_param) + \".keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T04:36:17.203281Z","iopub.execute_input":"2025-03-23T04:36:17.203614Z","iopub.status.idle":"2025-03-23T07:53:03.629663Z","shell.execute_reply.started":"2025-03-23T04:36:17.203586Z","shell.execute_reply":"2025-03-23T07:53:03.628910Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m871s\u001b[0m 157ms/step - auc: 0.7872 - loss: 0.6386 - val_auc: 0.9114 - val_loss: 0.3973\nEpoch 2/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 79ms/step - auc: 0.8756 - loss: 0.4590 - val_auc: 0.9201 - val_loss: 0.3798\nEpoch 3/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 55ms/step - auc: 0.8899 - loss: 0.4320 - val_auc: 0.9250 - val_loss: 0.3678\nEpoch 4/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 54ms/step - auc: 0.8982 - loss: 0.4159 - val_auc: 0.9283 - val_loss: 0.3585\nEpoch 5/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 56ms/step - auc: 0.9017 - loss: 0.4088 - val_auc: 0.9307 - val_loss: 0.3515\nEpoch 6/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 84ms/step - auc: 0.9053 - loss: 0.3995 - val_auc: 0.9331 - val_loss: 0.3456\nEpoch 7/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m589s\u001b[0m 107ms/step - auc: 0.9077 - loss: 0.3934 - val_auc: 0.9348 - val_loss: 0.3397\nEpoch 8/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 113ms/step - auc: 0.9102 - loss: 0.3881 - val_auc: 0.9355 - val_loss: 0.3373\nEpoch 9/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 78ms/step - auc: 0.9118 - loss: 0.3838 - val_auc: 0.9369 - val_loss: 0.3328\nEpoch 10/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 69ms/step - auc: 0.9109 - loss: 0.3846 - val_auc: 0.9378 - val_loss: 0.3295\nEpoch 1/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 55ms/step - auc: 0.7828 - loss: 0.8827 - val_auc: 0.9117 - val_loss: 0.5860\nEpoch 2/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 56ms/step - auc: 0.8769 - loss: 0.6281 - val_auc: 0.9218 - val_loss: 0.4957\nEpoch 3/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 55ms/step - auc: 0.8943 - loss: 0.5285 - val_auc: 0.9272 - val_loss: 0.4323\nEpoch 4/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 89ms/step - auc: 0.9010 - loss: 0.4711 - val_auc: 0.9303 - val_loss: 0.3958\nEpoch 5/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 57ms/step - auc: 0.9040 - loss: 0.4399 - val_auc: 0.9340 - val_loss: 0.3712\nEpoch 6/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 56ms/step - auc: 0.9056 - loss: 0.4224 - val_auc: 0.9357 - val_loss: 0.3561\nEpoch 7/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 57ms/step - auc: 0.9084 - loss: 0.4082 - val_auc: 0.9363 - val_loss: 0.3473\nEpoch 8/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 55ms/step - auc: 0.9126 - loss: 0.3941 - val_auc: 0.9390 - val_loss: 0.3364\nEpoch 9/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 57ms/step - auc: 0.9133 - loss: 0.3896 - val_auc: 0.9398 - val_loss: 0.3313\nEpoch 10/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 58ms/step - auc: 0.9138 - loss: 0.3862 - val_auc: 0.9407 - val_loss: 0.3293\nEpoch 1/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 57ms/step - auc: 0.7871 - loss: 2.6189 - val_auc: 0.9152 - val_loss: 0.9282\nEpoch 2/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 57ms/step - auc: 0.8821 - loss: 0.8206 - val_auc: 0.9254 - val_loss: 0.4586\nEpoch 3/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 59ms/step - auc: 0.8980 - loss: 0.4852 - val_auc: 0.9311 - val_loss: 0.3758\nEpoch 4/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 94ms/step - auc: 0.9047 - loss: 0.4233 - val_auc: 0.9321 - val_loss: 0.3587\nEpoch 5/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m527s\u001b[0m 96ms/step - auc: 0.9051 - loss: 0.4123 - val_auc: 0.9335 - val_loss: 0.3552\nEpoch 6/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 87ms/step - auc: 0.9074 - loss: 0.4062 - val_auc: 0.9347 - val_loss: 0.3491\nEpoch 7/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 72ms/step - auc: 0.9090 - loss: 0.4017 - val_auc: 0.9345 - val_loss: 0.3509\nEpoch 8/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 59ms/step - auc: 0.9079 - loss: 0.4047 - val_auc: 0.9357 - val_loss: 0.3466\nEpoch 9/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 56ms/step - auc: 0.9093 - loss: 0.4021 - val_auc: 0.9357 - val_loss: 0.3480\nEpoch 10/10\n\u001b[1m5501/5501\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 56ms/step - auc: 0.9096 - loss: 0.4008 - val_auc: 0.9365 - val_loss: 0.3475\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"In the following section, we discuss how model performance changed in response to different regularization parameter values.","metadata":{}},{"cell_type":"markdown","source":"# Results and analysis\n","metadata":{}},{"cell_type":"markdown","source":"We measured classification performance by area under each model's ROC curve. The following graphs show evolution in the first and second models' classification performance on training and validation images over successive training epochs.\n\n![Model 1 classification performance](https://github.com/mattjanko/cancer_detection/blob/main/model1.png?raw=true) \n![Model 2 classification performance](https://github.com/mattjanko/cancer_detection/blob/main/model2.png?raw=true)\n\nFor both models, classification performance on both training and validation data levels off within about ten epochs. Interestingly, both models tended to have more reliable classifications on the validation images than on the training images. This might be the result of an unusual characteristic of the training/validation split (e.g., the validation set was unusually homogeneous). A question for further investigation is whether the model continues to perform better on validation data given different training/validation splits. If this were the case, it might be a sign that the model is configured incorrectly.\n\nThe second model with its pretrained image extractor appears to have a stable advantage over the first model in terms of classification performance. However, another consideration is the speed with which the models are trained. The total training time for the two models is shown in the figure below.\n\n![Training times for the two models](https://github.com/mattjanko/cancer_detection/blob/main/timing.png?raw=true)\n\nThe second model took almost twice as much time to train as the first. Then again, the second model had more than fifteen times the number of parameters describing the connection between its convolutional block and its hidden layers, so relative to the number parameters, training the second model was actually more efficient.\n\nThe following graph shows model performance for each L2 regularization factor $\\lambda$.\n\n![Classification performance](https://github.com/mattjanko/cancer_detection/blob/main/tuning.png?raw=true)\n\nChanging the regularization factor did not have a dramatic effect on classification performance. However, when the value was set at $0.01$, classification performance over the ten epochs appeared to flatten more quickly, and when the value was set at $0.0001$, classification performance was consistently lower than when the value was wet at $0.001$. This suggests that an L2 regularization factor of $0.001$ will tend to lead to stronger classification performance on novel images.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nWe developed two models for classifying images of cell patches as canerous or non-cancerous. One model had a simple, custom-built convolutional block, and the other used a pretrained feature extraction model instead. Both models featured two additional hidden layers and an output layer with the same architecture. According to classification performance as measured by area under the models' ROC curves, the pretrained feature extractor led to the better model, and when we fine tuned the L2 regularization parameter used in that model, we found that $0.001$ is a reasonable value to use. The following code block loads the top model, preprocesses a batch of images not seen by the model during training or validation, and classifies those images.","metadata":{}},{"cell_type":"code","source":"best_model = tf.keras.models.load_model(\"/kaggle/working/cnn_pretrained.keras\")\ndf_test = pd.read_csv(path + \"sample_submission.csv\")\ndf_test[\"id\"] = df_test[\"id\"] + \".tif\"\ntestgen = ImageDataGenerator(rescale = 1/255)\ntest_loader = testgen.flow_from_dataframe(\n    dataframe = df_test,\n    directory = path + \"test/\",\n    x_col = \"id\",\n    class_mode = None, \n    target_size = (96, 96)\n)\nprediction = best_model.predict(test_loader)\nsubmission = pd.read_csv(path + \"sample_submission.csv\")\nsubmission[\"label\"] = prediction\nsubmission.to_csv(\"submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T23:40:28.972793Z","iopub.execute_input":"2025-03-26T23:40:28.973113Z","iopub.status.idle":"2025-03-26T23:42:14.848602Z","shell.execute_reply.started":"2025-03-26T23:40:28.973093Z","shell.execute_reply":"2025-03-26T23:42:14.847652Z"}},"outputs":[{"name":"stdout","text":"Found 57458 validated image filenames.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1796/1796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 42ms/step\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"When tested on the novel images, the area under the model's ROC curve was $0.8335$.\n\n![Leaderboard](https://github.com/mattjanko/cancer_detection/blob/main/leaderboard.png?raw=true)\n\nThere were a few key takeaways from this project.\n\n1. Relatively simple convolutional neural networks can be powerful and efficient classifiers. Our first model with a custom-trained convolutional block had only a small fraction of the trainable parameters of the second model with its pretrained feature extractor. Even so, the smaller model exhibited approximately $93\\%$ of the classification performance of the larger model as measured by area under ROC curves, and this result was achieved in approximately half the training time.\n2. L2 regularization probably did not contribute significantly to classification performance of the second model. We tested three regularization factors, $0.0001$, $0.001$, and $0.01$ and achieved slightly better performance with the intermediate value $0.001$. This suggests that classification performance is maximized locally at some value between $0.0001$ and $0.01$. However, there was only negligible change in the area under the model's ROC curve in response to changes in the regularization factor. Perhaps this is to be expected. Regularization is a countermeasure against possible overfitting, and there were other aspects of the model architecture (batch normalization and dropout) that serve the same purpose. This might explain why model performance did not seem to be affected much by changes to the regularization factor.\n3. Efficient hyperparameter tuning requires subjective judgements. Fine tuning just the regularization factor requried a couple hours of training time and had only minimal impact on model performance. Further refinements to the model would probably require fine tuning several hyperparameters simultaneously, which is extremely computationally expensive. One way to overcome this expense is to tune hyperparemeters over a small number of epochs and observe the early trends in classification performance. This is not as reliable as a tuning procedure over a large number of epochs, but many epochs can be prohibitively time consuming.","metadata":{}}]}